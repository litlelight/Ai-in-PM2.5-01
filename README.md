# PM-STGformer: Dual-Stage Attention Graph Transformer for PM2.5 Prediction with Chemical Interaction and Uncertainty Modeling

## ğŸš€ Overview

**PM-STGformer** is a novel spatiotemporal graph Transformer model that revolutionizes PM2.5 concentration prediction by explicitly modeling chemical interactions between pollutants and providing uncertainty quantification. This repository contains the complete implementation of our paper **"Dual-Stage Attention Graph Transformer for PM2.5 Prediction with Chemical Interaction and Uncertainty Modeling"**.

### ğŸ† Key Achievements
- **8.7%, 7.5%, and 5.2%** improvements in RMSE, MAE, and RÂ² metrics over state-of-the-art methods
- **>85%** prediction accuracy maintained during severe pollution episodes (PM2.5 > 250 Î¼g/mÂ³)
- **89% and 94%** coverage rates at 90% and 95% confidence levels for uncertainty estimation
- **80.6%** average performance retention in cross-city zero-shot transfer experiments

## ğŸ§  Technical Innovation

### Dual-Stage Attention Mechanism
- **Pollutant Association Module (PAM)**: Explicitly models complex chemical interactions between atmospheric pollutants through enhanced multi-head attention
- **Spatiotemporal Feature Fusion Module (STFM)**: Captures multi-scale temporal dependencies from hourly variations to seasonal trends

### Probabilistic Prediction Framework
- First PM2.5 prediction model to provide both point estimates and uncertainty quantification
- Gaussian negative log-likelihood loss for joint optimization of accuracy and uncertainty calibration
- Enables risk-based decision making for air quality management

### Graph-Enhanced Architecture
- Dynamic heterogeneous graph construction for pollutant interaction networks
- Multi-scale graph convolution with K-hop message passing
- Integration of atmospheric chemistry prior knowledge

## ğŸ“Š Model Architecture

```
Input Features â†’ Feature Embedding â†’ PAM â†’ STFM â†’ Multi-Scale Extraction â†’ Graph Construction â†’ Probabilistic Prediction Head
     â†“                 â†“              â†“      â†“           â†“                    â†“                    â†“
16 Features      256-dim Hidden   Chemical  Temporal   {3,5,7} Kernels    Dynamic Graph      Mean Â± Uncertainty
(6 Pollutants +   Representation  Interaction Attention  Multi-Scale       Convolution        PM2.5 Prediction
10 Meteorological)                Modeling   Mechanism   Features         Operations
```

## ğŸ”§ Installation

### Requirements
```bash
Python >= 3.8
PyTorch >= 2.0.0
NumPy >= 1.24.0
Pandas >= 2.0.0
Scikit-learn >= 1.3.0
```

### Setup
```bash
git clone https://github.com/litlelight/Ai-in-PM2.5-01.git
cd Ai-in-PM2.5-01
pip install -r requirements.txt
```

## ğŸ“ Repository Structure

```
PM-STGformer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ pm_stgformer.py          # Main model architecture
â”‚   â”‚   â”œâ”€â”€ pollutant_association.py # PAM module implementation
â”‚   â”‚   â”œâ”€â”€ spatiotemporal_fusion.py # STFM module implementation
â”‚   â”‚   â””â”€â”€ graph_constructor.py     # Dynamic graph construction
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ preprocessor.py          # Data preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ dataset.py               # PyTorch dataset implementation
â”‚   â”‚   â””â”€â”€ feature_engineering.py   # Feature extraction utilities
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py               # Training loop implementation
â”‚   â”‚   â”œâ”€â”€ losses.py                # Probabilistic loss functions
â”‚   â”‚   â””â”€â”€ metrics.py               # Evaluation metrics
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ visualization.py         # Result visualization tools
â”‚       â””â”€â”€ uncertainty_analysis.py  # Uncertainty calibration tools
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ beijing_aqi_2013_2017.csv    # Beijing air quality dataset
â”‚   â”œâ”€â”€ cross_city_data/             # Shanghai, Guangzhou, Chengdu data
â”‚   â””â”€â”€ preprocessed/                # Processed datasets
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ configs/                     # Experiment configurations
â”‚   â”œâ”€â”€ results/                     # Experimental results
â”‚   â””â”€â”€ notebooks/                   # Analysis notebooks
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pretrained/                  # Pre-trained model weights
â”‚   â””â”€â”€ checkpoints/                 # Training checkpoints
â””â”€â”€ docs/
    â”œâ”€â”€ API_documentation.md         # Detailed API documentation
    â”œâ”€â”€ experiment_reproduction.md   # Steps to reproduce results
    â””â”€â”€ model_architecture.md        # Detailed architecture explanation
```

## ğŸš€ Quick Start

### 1. Data Preparation
```bash
python src/data/preprocessor.py --dataset beijing --output_dir data/preprocessed/
```

### 2. Model Training
```bash
python src/training/trainer.py --config experiments/configs/beijing_config.yaml
```

### 3. Evaluation
```bash
python evaluate.py --model_path models/pretrained/pm_stgformer_beijing.pth --test_data data/preprocessed/beijing_test.pkl
```

### 4. Inference with Uncertainty
```python
from src.models.pm_stgformer import PMSTGformer
import torch

# Load pre-trained model
model = PMSTGformer.load_from_checkpoint('models/pretrained/pm_stgformer_beijing.pth')
model.eval()

# Make predictions with uncertainty
with torch.no_grad():
    mean_pred, uncertainty = model(input_data)
    confidence_interval = (mean_pred - 1.96 * uncertainty, mean_pred + 1.96 * uncertainty)
```

## ğŸ“ˆ Experimental Results

### Comparative Performance (Beijing Agricultural Exhibition Hall Station)

| Method | RMSE (Î¼g/mÂ³) | MAE (Î¼g/mÂ³) | MAPE (%) | RÂ² |
|--------|--------------|-------------|----------|-----|
| Linear Regression | 48.23 | 35.84 | 32.15 | 0.623 |
| XGBoost | 32.84 | 24.23 | 21.67 | 0.741 |
| LSTM | 29.45 | 21.38 | 19.82 | 0.778 |
| CNN-LSTM | 27.16 | 19.73 | 18.45 | 0.801 |
| AirFormer | 26.18 | 18.92 | 17.83 | 0.815 |
| ST-iTransformer | 24.92 | 17.84 | 16.81 | 0.832 |
| **PM-STGformer** | **23.60** | **16.82** | **15.23** | **0.847** |

### Uncertainty Calibration Results
- **90% Confidence Level**: 89.0% actual coverage (ideal: 90%)
- **95% Confidence Level**: 94.0% actual coverage (ideal: 95%)
- **Reliability Index**: 0.945 (closer to 1.0 indicates better calibration)

### Cross-City Generalization
| City | Zero-shot RMSE | 20% Fine-tuned RMSE | Retention Rate |
|------|---------------|---------------------|----------------|
| Shanghai | 31.20 | 25.92 | 81.4% |
| Guangzhou | 28.80 | 22.95 | 76.7% |
| Chengdu | 34.50 | 29.03 | 83.8% |

## ğŸ¯ Ablation Study Results

| Component Removed | RMSE (Î¼g/mÂ³) | Performance Drop |
|-------------------|--------------|------------------|
| Full Model | 23.60 | - |
| w/o STFM | 26.20 | +11.0% |
| w/o PAM | 25.80 | +9.3% |
| w/o Multi-scale | 25.11 | +6.4% |
| w/o Uncertainty | 24.31 | +3.0% |

## ğŸ“Š Dataset Information

### Beijing Air Quality Dataset
- **Time Period**: March 2013 - February 2017
- **Frequency**: Hourly measurements
- **Total Records**: 35,063 observations
- **Features**: 16 variables (6 pollutants + 10 meteorological)
- **Missing Data**: 2.3% (handled via temporal interpolation)

### Pollutants
- PM2.5, PM10, SOâ‚‚, NOâ‚‚, Oâ‚ƒ, CO

### Meteorological Variables
- Temperature (TEMP), Pressure (PRES), Dew Point (DEWP)
- Precipitation (RAIN), Wind Direction (wd), Wind Speed (WSPM)

## ğŸ”¬ Reproducibility

### Hardware Requirements
- **GPU**: NVIDIA RTX 4090 (24GB VRAM) or equivalent
- **RAM**: 128GB recommended for full dataset processing
- **Storage**: 50GB available space

### Training Configuration
- **Batch Size**: 512
- **Learning Rate**: 1e-3 with cosine annealing
- **Epochs**: 200 with early stopping
- **Optimizer**: Adam (Î²â‚=0.9, Î²â‚‚=0.999)
- **Training Time**: ~6 hours on dual RTX 4090

### Reproducing Results
```bash
# Run complete experimental pipeline
bash scripts/reproduce_all_experiments.sh

# Individual experiments
python experiments/run_comparative_study.py
python experiments/run_ablation_study.py
python experiments/run_cross_city_validation.py
python experiments/run_uncertainty_analysis.py
```

## ğŸ“‹ Model Configuration

### Optimal Hyperparameters (from ablation study)
```yaml
model:
  hidden_dim: 256
  num_heads: 4
  seq_len: 36  # hours
  dropout: 0.1
  kernel_sizes: [3, 5, 7]

training:
  loss_alpha: 0.5  # MSE-NLL balance
  learning_rate: 1e-3
  weight_decay: 1e-5
  gradient_clip: 1.0
```

## ğŸ¨ Visualization Tools

The repository includes comprehensive visualization utilities:

```bash
# Generate attention weight visualizations
python src/utils/visualization.py --type attention_weights --model_path models/pretrained/

# Plot prediction vs. ground truth with uncertainty bands
python src/utils/visualization.py --type prediction_analysis --test_data data/test/

# Uncertainty calibration plots
python src/utils/uncertainty_analysis.py --generate_plots
```

## ğŸ“š Citation

If you use this code or data in your research, please cite our paper:

```bibtex
@article{yang2024pmstgformer,
  title={Dual-Stage Attention Graph Transformer for PM2.5 Prediction with Chemical Interaction and Uncertainty Modeling},
  author={Yang, YanCheng and Zhang, YuChen},
  journal={arXiv preprint arXiv:xxxx.xxxxx},
  year={2024}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Code formatting
black src/
isort src/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Beijing Environmental Monitoring Center for providing air quality data
- National Meteorological Information Center for meteorological data
- PyTorch and Hugging Face communities for excellent deep learning frameworks

## ğŸ“ Contact

For questions about the code or paper, please contact:
- **YanCheng Yang**: [1192448328@qq.com](mailto:1192448328@qq.com)
- **YuChen Zhang**: [2627556529@qq.com](mailto:2627556529@qq.com) (Corresponding Author)

## ğŸ”„ Updates

- **v1.0.0** (2024-01): Initial release with complete PM-STGformer implementation
- **v1.1.0** (2024-02): Added cross-city validation experiments
- **v1.2.0** (2024-03): Enhanced uncertainty analysis tools

---

**Note**: This implementation represents the core research contribution. Production deployment may require additional optimization and calibration specific to local conditions and regulatory requirements.
